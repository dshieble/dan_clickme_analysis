{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import h5py\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import helper_functions as hf\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.misc import imread\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from sklearn.metrics import roc_auc_score, classification_report, accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import LinearSVC\n",
    "import pickledb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 32.22640351\n",
      "Iteration 2, loss = 29.06713270\n",
      "Iteration 3, loss = 26.81946577\n",
      "Iteration 4, loss = 24.90353096\n",
      "Iteration 5, loss = 22.98644952\n",
      "Iteration 6, loss = 21.36939154\n",
      "Iteration 7, loss = 19.94075052\n",
      "Iteration 8, loss = 18.57301044\n",
      "Iteration 9, loss = 17.35168126\n",
      "Iteration 10, loss = 16.20321374\n",
      "Iteration 11, loss = 15.32362569\n",
      "Iteration 12, loss = 14.36206239\n",
      "Iteration 13, loss = 13.54961761\n",
      "Iteration 14, loss = 12.77163964\n",
      "Iteration 15, loss = 12.17701404\n",
      "Iteration 16, loss = 11.50120402\n",
      "Iteration 17, loss = 10.92451357\n",
      "Iteration 18, loss = 10.41363841\n",
      "Iteration 19, loss = 9.95095426\n",
      "Iteration 20, loss = 9.50778995\n",
      "Iteration 21, loss = 9.11046546\n",
      "Iteration 22, loss = 8.81099755\n",
      "Iteration 23, loss = 8.42224987\n",
      "Iteration 24, loss = 8.09328128\n",
      "Iteration 25, loss = 7.78360836\n",
      "Iteration 26, loss = 7.51748996\n",
      "Iteration 27, loss = 7.27276961\n",
      "Iteration 28, loss = 7.02545457\n",
      "Iteration 29, loss = 6.82890013\n",
      "Iteration 30, loss = 6.61674073\n",
      "Iteration 31, loss = 6.41899446\n",
      "Iteration 32, loss = 6.27119724\n",
      "Iteration 33, loss = 6.08754877\n",
      "Iteration 34, loss = 5.93810430\n",
      "Iteration 35, loss = 5.79159221\n",
      "Iteration 36, loss = 5.66449723\n",
      "Iteration 37, loss = 5.53495532\n",
      "Iteration 38, loss = 5.41022278\n",
      "Iteration 39, loss = 5.31103586\n",
      "Iteration 40, loss = 5.17602565\n",
      "Iteration 41, loss = 5.08404500\n",
      "Iteration 42, loss = 4.98654865\n",
      "Iteration 43, loss = 4.88920449\n",
      "Iteration 44, loss = 4.80714262\n",
      "Iteration 45, loss = 4.72323049\n",
      "Iteration 46, loss = 4.65119443\n",
      "Iteration 47, loss = 4.56022871\n",
      "Iteration 48, loss = 4.50220484\n",
      "Iteration 49, loss = 4.43563467\n",
      "Iteration 50, loss = 4.36294992\n",
      "Iteration 51, loss = 4.31271611\n",
      "Iteration 52, loss = 4.25248439\n",
      "Iteration 53, loss = 4.17951983\n",
      "Iteration 54, loss = 4.13001073\n",
      "Iteration 55, loss = 4.08371117\n",
      "Iteration 56, loss = 4.03343090\n",
      "Iteration 57, loss = 3.98579166\n",
      "Iteration 58, loss = 3.93527325\n",
      "Iteration 59, loss = 3.88879389\n",
      "Iteration 60, loss = 3.86317258\n",
      "Iteration 61, loss = 3.80887051\n",
      "Iteration 62, loss = 3.77133380\n",
      "Iteration 63, loss = 3.73460564\n",
      "Iteration 64, loss = 3.68608998\n",
      "Iteration 65, loss = 3.66888713\n",
      "Iteration 66, loss = 3.63426545\n",
      "Iteration 67, loss = 3.59320806\n",
      "Iteration 68, loss = 3.56039541\n",
      "Iteration 69, loss = 3.52307547\n",
      "Iteration 70, loss = 3.49439937\n",
      "Iteration 71, loss = 3.46550147\n",
      "Iteration 72, loss = 3.44712876\n",
      "Iteration 73, loss = 3.40377667\n",
      "Iteration 74, loss = 3.39328446\n",
      "Iteration 75, loss = 3.36205926\n",
      "Iteration 76, loss = 3.33455550\n",
      "Iteration 77, loss = 3.31216315\n",
      "Iteration 78, loss = 3.30513977\n",
      "Iteration 79, loss = 3.27099335\n",
      "Iteration 80, loss = 3.23416683\n",
      "Iteration 81, loss = 3.21958125\n",
      "Iteration 82, loss = 3.19093025\n",
      "Iteration 83, loss = 3.16960329\n",
      "Iteration 84, loss = 3.16614881\n",
      "Iteration 85, loss = 3.15136264\n",
      "Iteration 86, loss = 3.11918935\n",
      "Iteration 87, loss = 3.09582748\n",
      "Iteration 88, loss = 3.08071427\n",
      "Iteration 89, loss = 3.06200645\n",
      "Iteration 90, loss = 3.04756367\n",
      "Iteration 91, loss = 3.03423118\n",
      "Iteration 92, loss = 3.02861497\n",
      "Iteration 93, loss = 2.99918095\n",
      "Iteration 94, loss = 2.99137962\n",
      "Iteration 95, loss = 2.96862695\n",
      "Iteration 96, loss = 2.94716191\n",
      "Iteration 97, loss = 2.93847337\n",
      "Iteration 98, loss = 2.92426264\n",
      "Iteration 99, loss = 2.91756892\n",
      "Iteration 100, loss = 2.91628815\n",
      "Iteration 101, loss = 2.87646202\n",
      "Iteration 102, loss = 2.86806409\n",
      "Iteration 103, loss = 2.85424726\n",
      "Iteration 104, loss = 2.85512419\n",
      "Iteration 105, loss = 2.83945709\n",
      "Iteration 106, loss = 2.82823941\n",
      "Iteration 107, loss = 2.80100000\n",
      "Iteration 108, loss = 2.78919120\n",
      "Iteration 109, loss = 2.79413741\n",
      "Iteration 110, loss = 2.77995801\n",
      "Iteration 111, loss = 2.76661028\n",
      "Iteration 112, loss = 2.74194742\n",
      "Iteration 113, loss = 2.74029573\n",
      "Iteration 114, loss = 2.74443271\n",
      "Iteration 115, loss = 2.72654401\n",
      "Iteration 116, loss = 2.71231333\n",
      "Iteration 117, loss = 2.70596720\n",
      "Iteration 118, loss = 2.69818281\n",
      "Iteration 119, loss = 2.68169502\n",
      "Iteration 120, loss = 2.67089560\n",
      "Iteration 121, loss = 2.66141349\n",
      "Iteration 122, loss = 2.65045649\n",
      "Iteration 123, loss = 2.64988527\n",
      "Iteration 124, loss = 2.63869645\n",
      "Iteration 125, loss = 2.63938490\n",
      "Iteration 126, loss = 2.61372573\n",
      "Iteration 127, loss = 2.60952999\n",
      "Iteration 128, loss = 2.61434931\n",
      "Iteration 129, loss = 2.59254164\n",
      "Iteration 130, loss = 2.57318283\n",
      "Iteration 131, loss = 2.57919128\n",
      "Iteration 132, loss = 2.57001377\n",
      "Iteration 133, loss = 2.57488585\n",
      "Iteration 134, loss = 2.54852774\n",
      "Iteration 135, loss = 2.54872636\n",
      "Iteration 136, loss = 2.54636931\n",
      "Iteration 137, loss = 2.52959387\n",
      "Iteration 138, loss = 2.52549481\n",
      "Iteration 139, loss = 2.52348946\n",
      "Iteration 140, loss = 2.51028291\n",
      "Iteration 141, loss = 2.50708455\n",
      "Iteration 142, loss = 2.50354174\n",
      "Iteration 143, loss = 2.48301037\n",
      "Iteration 144, loss = 2.48759822\n",
      "Iteration 145, loss = 2.49348228\n",
      "Iteration 146, loss = 2.47364977\n",
      "Iteration 147, loss = 2.45886495\n",
      "Iteration 148, loss = 2.47982574\n",
      "Iteration 149, loss = 2.45290290\n",
      "Iteration 150, loss = 2.45127336\n",
      "Iteration 151, loss = 2.44980839\n",
      "Iteration 152, loss = 2.43781822\n",
      "Iteration 153, loss = 2.42187044\n",
      "Iteration 154, loss = 2.41861076\n",
      "Iteration 155, loss = 2.40853858\n",
      "Iteration 156, loss = 2.41118865\n",
      "Iteration 157, loss = 2.40246291\n",
      "Iteration 158, loss = 2.40279330\n",
      "Iteration 159, loss = 2.38943811\n",
      "Iteration 160, loss = 2.38338738\n",
      "Iteration 161, loss = 2.38037426\n",
      "Iteration 162, loss = 2.37583028\n",
      "Iteration 163, loss = 2.37449344\n",
      "Iteration 164, loss = 2.37041017\n",
      "Iteration 165, loss = 2.37435024\n",
      "Iteration 166, loss = 2.36509724\n",
      "Iteration 167, loss = 2.34563072\n",
      "Iteration 168, loss = 2.34104628\n",
      "Iteration 169, loss = 2.32907813\n",
      "Iteration 170, loss = 2.34746730\n",
      "Iteration 171, loss = 2.34036095\n",
      "Iteration 172, loss = 2.32758395\n",
      "Iteration 173, loss = 2.31772970\n",
      "Iteration 174, loss = 2.30967742\n",
      "Iteration 175, loss = 2.30905179\n",
      "Iteration 176, loss = 2.31229563\n",
      "Iteration 177, loss = 2.31640840\n",
      "Iteration 178, loss = 2.29605376\n",
      "Iteration 179, loss = 2.29421764\n",
      "Iteration 180, loss = 2.28406947\n",
      "Iteration 181, loss = 2.27888805\n",
      "Iteration 182, loss = 2.27733224\n",
      "Iteration 183, loss = 2.27661664\n",
      "Iteration 184, loss = 2.27529573\n",
      "Iteration 185, loss = 2.27805715\n",
      "Iteration 186, loss = 2.26753495\n",
      "Iteration 187, loss = 2.24997558\n",
      "Iteration 188, loss = 2.27161557\n",
      "Iteration 189, loss = 2.24407860\n",
      "Iteration 190, loss = 2.25166519\n",
      "Iteration 191, loss = 2.23790272\n",
      "Iteration 192, loss = 2.24275168\n",
      "Iteration 193, loss = 2.24286979\n",
      "Iteration 194, loss = 2.22755794\n",
      "Iteration 195, loss = 2.24291955\n",
      "Iteration 196, loss = 2.23304002\n",
      "Iteration 197, loss = 2.21112582\n",
      "Iteration 198, loss = 2.21363825\n",
      "Iteration 199, loss = 2.20569728\n",
      "Iteration 200, loss = 2.21549956\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/danshiebler/env/local/lib/python2.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:563: ConvergenceWarning: Stochastic Optimizer: Maximum iterations reached and the optimization hasn't converged yet.\n",
      "  % (), ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='logistic', alpha=0.0001, batch_size='auto',\n",
       "       beta_1=0.9, beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(), learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
       "       nesterovs_momentum=True, power_t=0.5, random_state=None,\n",
       "       shuffle=True, solver='adam', tol=0.0001, validation_fraction=0.1,\n",
       "       verbose=True, warm_start=False)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = MLPClassifier(hidden_layer_sizes=(),\n",
    "                                           activation='logistic', solver='adam', \n",
    "                                           alpha=0.0001, batch_size='auto', \n",
    "                                           learning_rate='constant', \n",
    "                                           learning_rate_init=0.001, power_t=0.5, \n",
    "                                           max_iter=200, shuffle=True, random_state=None, \n",
    "                                           tol=0.0001, verbose=True, warm_start=False, \n",
    "                                           momentum=0.9, nesterovs_momentum=True,\n",
    "                                           early_stopping=True, validation_fraction=0.1, \n",
    "                                           beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "clf.fit(Xtrain_scaled, ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.177803983229\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.36      0.27      0.31       279\n",
      "          1       0.55      0.41      0.47      1029\n",
      "          2       0.55      0.53      0.54       268\n",
      "          3       0.80      0.58      0.67       220\n",
      "          4       0.46      0.51      0.48       312\n",
      "          5       0.68      0.65      0.66       317\n",
      "          6       0.41      0.27      0.32       515\n",
      "          7       0.74      0.52      0.61       287\n",
      "          8       0.43      0.35      0.39       352\n",
      "          9       0.63      0.23      0.33        75\n",
      "         10       0.32      0.20      0.24       486\n",
      "         11       0.48      0.46      0.47       246\n",
      "         12       0.60      0.45      0.51       107\n",
      "         13       0.78      0.68      0.73       180\n",
      "         14       0.82      0.53      0.64        97\n",
      "         15       0.92      0.83      0.88       191\n",
      "         16       0.14      0.06      0.09       434\n",
      "         17       0.49      0.40      0.44       335\n",
      "         18       0.40      0.35      0.38       309\n",
      "         19       0.89      0.68      0.77       259\n",
      "         20       0.46      0.34      0.39       756\n",
      "         21       0.33      0.22      0.26       368\n",
      "         22       0.23      0.19      0.21       279\n",
      "         23       0.44      0.36      0.39       606\n",
      "         24       0.30      0.19      0.23       124\n",
      "         25       0.41      0.31      0.35       137\n",
      "         26       0.55      0.54      0.55       164\n",
      "         27       0.34      0.35      0.34       101\n",
      "         28       0.58      0.61      0.60       270\n",
      "         29       0.47      0.31      0.38      1096\n",
      "         30       0.40      0.28      0.33       383\n",
      "         31       0.70      0.68      0.69       320\n",
      "         32       0.51      0.40      0.45       412\n",
      "         33       0.46      0.39      0.42       311\n",
      "         34       0.47      0.45      0.46       173\n",
      "         35       0.17      0.10      0.13       256\n",
      "         36       0.48      0.50      0.49       194\n",
      "         37       0.29      0.18      0.23       438\n",
      "         38       0.25      0.22      0.23       116\n",
      "         39       0.00      0.00      0.00        21\n",
      "         40       0.38      0.27      0.32       191\n",
      "         41       0.36      0.24      0.29       479\n",
      "         42       0.59      0.45      0.51       404\n",
      "         43       0.40      0.30      0.34       309\n",
      "         44       0.50      0.41      0.45       175\n",
      "         45       0.50      0.05      0.09        20\n",
      "\n",
      "avg / total       0.47      0.37      0.41     14401\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions = clf.predict(Xtest_scaled)\n",
    "accuracy = accuracy_score(ytest, predictions)\n",
    "report = classification_report(ytest, predictions)\n",
    "print accuracy\n",
    "print report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading...\n"
     ]
    },
    {
     "ename": "IOError",
     "evalue": "[Errno 2] No such file or directory: 'data/1500301581.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-f054dd9b5132>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0;34m\"loading...\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0mfile_to_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"data/{}.npy\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0;34m\"loaded!\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/danshiebler/env/local/lib/python2.7/site-packages/numpy/lib/npyio.pyc\u001b[0m in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[1;32m    368\u001b[0m     \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbasestring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 370\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    371\u001b[0m         \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    372\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mis_pathlib_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIOError\u001b[0m: [Errno 2] No such file or directory: 'data/1500301581.npy'"
     ]
    }
   ],
   "source": [
    "# from sklearn.neural_network import MLPClassifier\n",
    "# clf = MLPClassifier(hidden_layer_sizes=(),\n",
    "#                                            activation='sigmoid', solver='adam', \n",
    "#                                            alpha=0.0001, batch_size='auto', \n",
    "#                                            learning_rate='constant', \n",
    "#                                            learning_rate_init=0.001, power_t=0.5, \n",
    "#                                            max_iter=200, shuffle=True, random_state=None, \n",
    "#                                            tol=0.0001, verbose=False, warm_start=False, \n",
    "#                                            momentum=0.9, nesterovs_momentum=True,\n",
    "#                                            early_stopping=False, validation_fraction=0.1, \n",
    "#                                            beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "\n",
    "# #TODO: WRITE A FUNCTION TO REMOVE THE CLASSES 39 AND 45 (AND POSSIBLY MORE) FROM ALL LABELS AND REMOVE ALL IMAGES \n",
    "# # THAT ONLY CONSIST OF THOSE CLASSES\n",
    "\n",
    "# features_db = pickledb.load('databases/signature_to_saved_data_path.db', True)\n",
    "# signature = \"1500301581\"\n",
    "\n",
    "# img_ids, img_ID_to_LABELS = hf.get_ids_labels()\n",
    "# img_ids = [ID for ID in img_ids if not img_ID_to_LABELS[ID] in [39, 45]]\n",
    "# labels = [img_ID_to_LABELS[ID] for ID in img_ids]\n",
    "\n",
    "# print \"loading...\"\n",
    "# file_to_features = np.load(\"data/{}.npy\".format(signature)).item()\n",
    "# print \"loaded!\"\n",
    "\n",
    "# X = np.vstack([file_to_features[ID] for ID in img_ids])\n",
    "# y = MultiLabelBinarizer().fit_transform([img_ID_to_LABELS[ID] for ID in img_ids])\n",
    "\n",
    "# indices = np.random.permutation(np.arange(X.shape[0]))\n",
    "# train, test = indices[:int(0.75*len(indices))], indices[int(0.75*len(indices)):]\n",
    "# Xtrain, ytrain, Xtest, ytest = X[train], y[train],  X[test], y[test]\n",
    "\n",
    "# scalar = StandardScaler()\n",
    "# Xtrain_scaled = scalar.fit_transform(Xtrain)\n",
    "# Xtest_scaled = scalar.transform(Xtest)\n",
    "# print Xtrain_scaled.shape, ytrain.shape, Xtest_scaled.shape, ytest.shape\n",
    "\n",
    "# start = time.time()\n",
    "# print \"training...\"\n",
    "# for epoch in tqdm(range(num_epochs)):\n",
    "#     clf.fit(Xtrain_scaled, ytrain)\n",
    "\n",
    "# print \"trained in {}!\".format(time.time() - start)\n",
    "\n",
    "# predictions = clf.predict(Xtest_scaled)\n",
    "# accuracy = accuracy_score(ytest, predictions)\n",
    "# report = classification_report(ytest, predictions)\n",
    "# print accuracy\n",
    "# print report\n",
    "# data = {\"accuracy\":accuracy, \"predictions\":list(predictions), \"y\":list(ytest), \"report\": report, \"num_epochs\":num_epochs, \"batch_size\":batch_size}\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.00      0.00      0.00       279\n",
      "          1       0.63      0.14      0.22      1029\n",
      "          2       0.89      0.16      0.27       268\n",
      "          3       1.00      0.16      0.28       220\n",
      "          4       0.85      0.18      0.30       312\n",
      "          5       1.00      0.25      0.39       317\n",
      "          6       0.57      0.03      0.05       515\n",
      "          7       1.00      0.05      0.09       287\n",
      "          8       0.79      0.03      0.06       352\n",
      "          9       0.00      0.00      0.00        75\n",
      "         10       0.67      0.00      0.01       486\n",
      "         11       1.00      0.04      0.09       246\n",
      "         12       1.00      0.02      0.04       107\n",
      "         13       1.00      0.35      0.52       180\n",
      "         14       1.00      0.01      0.02        97\n",
      "         15       1.00      0.54      0.70       191\n",
      "         16       0.25      0.00      0.00       434\n",
      "         17       1.00      0.01      0.03       335\n",
      "         18       0.96      0.09      0.16       309\n",
      "         19       0.98      0.33      0.49       259\n",
      "         20       0.56      0.06      0.11       756\n",
      "         21       0.43      0.01      0.02       368\n",
      "         22       0.33      0.01      0.01       279\n",
      "         23       0.48      0.04      0.07       606\n",
      "         24       0.00      0.00      0.00       124\n",
      "         25       1.00      0.03      0.06       137\n",
      "         26       0.83      0.12      0.20       164\n",
      "         27       1.00      0.01      0.02       101\n",
      "         28       0.83      0.14      0.24       270\n",
      "         29       0.55      0.07      0.12      1096\n",
      "         30       0.50      0.01      0.02       383\n",
      "         31       0.91      0.19      0.31       320\n",
      "         32       0.62      0.06      0.11       412\n",
      "         33       0.89      0.05      0.10       311\n",
      "         34       0.43      0.03      0.06       173\n",
      "         35       0.00      0.00      0.00       256\n",
      "         36       0.80      0.04      0.08       194\n",
      "         37       0.00      0.00      0.00       438\n",
      "         38       0.00      0.00      0.00       116\n",
      "         39       0.00      0.00      0.00        21\n",
      "         40       0.50      0.01      0.01       191\n",
      "         41       0.50      0.01      0.02       479\n",
      "         42       1.00      0.11      0.20       404\n",
      "         43       1.00      0.00      0.01       309\n",
      "         44       0.00      0.00      0.00       175\n",
      "         45       0.00      0.00      0.00        20\n",
      "\n",
      "avg / total       0.63      0.07      0.12     14401\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/danshiebler/env/local/lib/python2.7/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# clf = RandomForestClassifier()#OneVsRestClassifier(SGDClassifier(warm_start=True))#LogisticRegression(solver='sag', multi_class=\"multinomial\", verbose=1000, n_jobs=-1)\n",
    "# clf.fit(Xtrain_scaled, ytrain)\n",
    "predictions = clf.predict(Xtest_scaled)\n",
    "report = classification_report(ytest, predictions)\n",
    "print report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# signature = \"1500301581\"\n",
    "# clf = OneVsRestClassifier(SGDClassifier(loss=\"log\", warm_start=True, n_jobs=-1))#LogisticRegression(solver='sag', multi_class=\"multinomial\", verbose=1000, n_jobs=-1)\n",
    "\n",
    "\n",
    "\n",
    "# features_db = pickledb.load('databases/signature_to_saved_data_path.db', True)\n",
    "# model_results_db = pickledb.load('databases/signature_to_linear_model_results.db', True)\n",
    "\n",
    "\n",
    "# print \"prediction for {}\".format(features_db.get(signature)[\"saved_data_path\"])\n",
    "\n",
    "# features_db = pickledb.load('databases/signature_to_saved_data_path.db', True)\n",
    "# signature = \"1500301581\"\n",
    "\n",
    "# img_ids, img_ID_to_LABELS = hf.get_ids_labels()\n",
    "# # img_ids = [ID for ID in img_ids if not img_ID_to_LABELS[ID] in [39, 45]]\n",
    "# labels = [img_ID_to_LABELS[ID] for ID in img_ids]\n",
    "\n",
    "# print \"loading...\"\n",
    "# file_to_features = np.load(\"data/{}.npy\".format(signature)).item()\n",
    "# print \"loaded!\"\n",
    "\n",
    "# X = np.vstack([file_to_features[ID] for ID in img_ids])\n",
    "# y = MultiLabelBinarizer().fit_transform([img_ID_to_LABELS[ID] for ID in img_ids])\n",
    "\n",
    "# indices = np.random.permutation(np.arange(X.shape[0]))\n",
    "# train, test = indices[:int(0.75*len(indices))], indices[int(0.75*len(indices)):]\n",
    "# Xtrain, ytrain, Xtest, ytest = X[train], y[train],  X[test], y[test]\n",
    "\n",
    "# scalar = StandardScaler()\n",
    "# Xtrain_scaled = scalar.fit_transform(Xtrain)\n",
    "# Xtest_scaled = scalar.transform(Xtest)\n",
    "# print Xtrain_scaled.shape, ytrain.shape, Xtest_scaled.shape, ytest.shape\n",
    "\n",
    "\n",
    "# start = time.time()\n",
    "# print \"training...\"\n",
    "# clf.fit(Xtrain_scaled, ytrain)\n",
    "# print \"trained in {}!\".format(time.time() - start)\n",
    "\n",
    "# predictions = clf.predict(Xtest_scaled)\n",
    "# probs = clf.predict_proba(Xtest_scaled)\n",
    "\n",
    "# accuracy = accuracy_score(ytest, predictions)\n",
    "# recall = recall_score(ytest, predictions, average=\"samples\")\n",
    "# precision = precision_score(ytest, predictions, average=\"samples\")\n",
    "\n",
    "# report = classification_report(ytest, predictions)\n",
    "# print \"accuracy\", accuracy\n",
    "# print \"recall\", recall\n",
    "# print \"precision\", precision\n",
    "\n",
    "# data = {\"accuracy\":accuracy, \"recall\":recall, \"precision\":precision, \"predictions\":predictions.tolist(), \n",
    "#         \"probs\":probs.tolist(), \"y\":ytest.tolist(), \"report\": report}\n",
    "# hf.save_to_db(model_results_db, signature, data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "probability estimates are not available for loss='hinge'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-c0f3ed994234>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXtest_scaled\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXtest_scaled\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mytest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mrecall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrecall_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mytest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"samples\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/danshiebler/env/local/lib/python2.7/site-packages/sklearn/multiclass.pyc\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    300\u001b[0m             \u001b[0margmaxima\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimators_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 302\u001b[0;31m                 \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_predict_binary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    303\u001b[0m                 \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaximum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaxima\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmaxima\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m                 \u001b[0margmaxima\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmaxima\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/danshiebler/env/local/lib/python2.7/site-packages/sklearn/multiclass.pyc\u001b[0m in \u001b[0;36m_predict_binary\u001b[0;34m(estimator, X)\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mAttributeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0;31m# probabilities of the positive class\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m         \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/danshiebler/env/local/lib/python2.7/site-packages/sklearn/linear_model/stochastic_gradient.pyc\u001b[0m in \u001b[0;36mpredict_proba\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    760\u001b[0m         \u001b[0mhttp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0mjmlr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsail\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0medu\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mpapers\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mvolume2\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mzhang02c\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mzhang02c\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    761\u001b[0m         \"\"\"\n\u001b[0;32m--> 762\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    763\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_predict_proba\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    764\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/danshiebler/env/local/lib/python2.7/site-packages/sklearn/linear_model/stochastic_gradient.pyc\u001b[0m in \u001b[0;36m_check_proba\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    722\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"log\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"modified_huber\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    723\u001b[0m             raise AttributeError(\"probability estimates are not available for\"\n\u001b[0;32m--> 724\u001b[0;31m                                  \" loss=%r\" % self.loss)\n\u001b[0m\u001b[1;32m    725\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: probability estimates are not available for loss='hinge'"
     ]
    }
   ],
   "source": [
    "predictions = clf.predict(Xtest_scaled)\n",
    "probs = clf.predict_proba(Xtest_scaled)\n",
    "\n",
    "accuracy = accuracy_score(ytest, predictions)\n",
    "recall = recall_score(ytest, predictions, average=\"samples\")\n",
    "precision = precision_score(ytest, predictions, average=\"samples\")\n",
    "\n",
    "report = classification_report(ytest, predictions)\n",
    "print \"accuracy\", accuracy\n",
    "print \"recall\", recall\n",
    "print \"precision\", precision\n",
    "\n",
    "data = {\"accuracy\":accuracy, \"recall\":recall, \"precision\":precision, \"predictions\":predictions.tolist(), \n",
    "        \"probs\":probs.tolist(), \"y\":ytest.tolist(), \"report\": report, \"batch_size\":batch_size}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The object was not fitted with multilabel input.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-f4ad83112be9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0;34m\"training...\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartial_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXtrain_scaled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mytrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mytrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0;34m\"trained in {}!\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/danshiebler/env/local/lib/python2.7/site-packages/sklearn/multiclass.pyc\u001b[0m in \u001b[0;36mpartial_fit\u001b[0;34m(self, X, y, classes)\u001b[0m\n\u001b[1;32m    263\u001b[0m                                                              self.classes_))\n\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 265\u001b[0;31m         \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel_binarizer_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    266\u001b[0m         \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtocsc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m         \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/danshiebler/env/local/lib/python2.7/site-packages/sklearn/preprocessing/label.pyc\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, y)\u001b[0m\n\u001b[1;32m    327\u001b[0m         \u001b[0my_is_multilabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'multilabel'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my_is_multilabel\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my_type_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'multilabel'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 329\u001b[0;31m             raise ValueError(\"The object was not fitted with multilabel\"\n\u001b[0m\u001b[1;32m    330\u001b[0m                              \" input.\")\n\u001b[1;32m    331\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: The object was not fitted with multilabel input."
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(solver='sag', multi_class=\"multinomial\", verbose=1000, n_jobs=-1)\n",
    "batch_size = 50000\n",
    "num_epochs = 10\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "print \"training...\"\n",
    "clf.fit(Xtrain_scaled, ytrain, classes=np.arange(ytrain.shape[1]))\n",
    "print \"trained in {}!\".format(time.time() - start)\n",
    "\n",
    "predictions = clf.predict(Xtest_scaled)\n",
    "accuracy = accuracy_score(ytest, predictions)\n",
    "report = classification_report(ytest, predictions)\n",
    "print accuracy\n",
    "print report\n",
    "data = {\"accuracy\":accuracy, \"predictions\":list(predictions), \"y\":list(ytest), \"report\": report, \"num_epochs\":num_epochs, \"batch_size\":batch_size}\n",
    "# hf.save_to_db(model_results_db, signature, data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ..., \n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ytrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "L = !ls /media/data_cifs/image_datasets/coco_2014/coco_images/ilsvrc12_val_overlap\n",
    "num = '339'\n",
    "overlap_num = [l.split(\"_\")[0] + l.split(\"_\")[1].split(\".\")[0] for l in L  if l.split(\"_\")[0] == num]\n",
    "train_num = set([str(k) for k in train_coco.imgs.keys() if str(k)[:3] == num])\n",
    "val_num = set([str(k) for k in val_coco.imgs.keys() if str(k)[:3] == num])\n",
    "IN = [k for k in overlap_num if (k in train_num or k in val_num)]\n",
    "OUT = [k for k in overlap_num if not (k in train_num or k in val_num)]\n",
    "len(IN), len(OUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "path = \"/media/data_cifs/image_datasets/coco_2014/coco_images\"\n",
    "ilsvrc_overlap = pd.read_csv(\"{}/ilsvrc_overlap.csv\".format(path))\n",
    "ilsvrc_coco_overlap_categories = np.load(\"{}/ilsvrc_coco_overlap_categories.npy\".format(path))\n",
    "coco_full_im_processed_labels = np.load(\"{}/coco_full_im_processed_labels.npz\".format(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "coco_full_im_processed_labels[\"training_image_map\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "features_db = pickledb.load('databases/signature_to_saved_data_path.db', True)\n",
    "model_results_db = pickledb.load('databases/signature_to_linear_model_results.db', True)\n",
    "for k in features_db.getall():\n",
    "    if model_results_db.get(k):\n",
    "        print k\n",
    "        print features_db.get(k)['saved_data_path']\n",
    "        print model_results_db.get(k)[\"accuracy\"]\n",
    "        print "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "label_inds, label_onehots, data_files = hf.get_files_labels(load_small=False)\n",
    "file_to_labels = dict(zip(data_files, label_inds))\n",
    "print \"loading...\"\n",
    "file_to_features = np.load(\"data/1500173639.npy\").item()\n",
    "print \"loaded!\"\n",
    "\n",
    "X = np.vstack([v for v in file_to_features.values()])\n",
    "y = np.array([file_to_labels[k] for k in file_to_features.keys()])\n",
    "\n",
    "indices = np.random.permutation(np.arange(X.shape[0]))\n",
    "train, test = indices[:int(0.75*len(indices))], indices[int(0.75*len(indices)):]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "clf = LogisticRegression(solver='sag', multi_class=\"multinomial\", verbose=1000, n_jobs=16)\n",
    "# clf = LinearSVC(dual=False, verbose = 1000)\n",
    "\n",
    "\n",
    "scalar = StandardScaler()\n",
    "pipeline = Pipeline([(\"pp\", scalar), (\"clf\", clf)])\n",
    "\n",
    "start = time.time()\n",
    "print \"training...\"\n",
    "pipeline.fit(X[train], y[train])\n",
    "print \"trained in {}!\".format(time.time() - start)\n",
    "\n",
    "predictions = pipeline.predict(X[test])\n",
    "print accuracy_score(y[test], predictions)\n",
    "print classification_report(y[test], predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
