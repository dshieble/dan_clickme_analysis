{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from scipy.misc import imread, imresize\n",
    "from scipy.misc import imsave\n",
    "import helper_functions as hf\n",
    "import pickledb\n",
    "\n",
    "def hamming_score(y_true, y_pred, normalize=True, sample_weight=None):\n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    acc_list = []\n",
    "    for i in range(y_true.shape[0]):\n",
    "        set_true = set( np.where(y_true[i])[0] )\n",
    "        set_pred = set( np.where(y_pred[i])[0] )\n",
    "        tmp_a = None\n",
    "        if len(set_true) == 0 and len(set_pred) == 0:\n",
    "            tmp_a = 1\n",
    "        else:\n",
    "            tmp_a = len(set_true.intersection(set_pred))/\\\n",
    "                    float( len(set_true.union(set_pred)) )\n",
    "        #print('tmp_a: {0}'.format(tmp_a))\n",
    "        acc_list.append(tmp_a)\n",
    "    return np.mean(acc_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/attention_gradient_checkpoints/gradient_001_130671_2017_07_15_15_01_12/model_44000.ckpt-44000\n",
      "average recall 0.465387851198\n",
      "average precision 0.623378801025\n",
      "hamming score 0.400511474997\n",
      "\n",
      "/checkpoints/baseline_001_50000_2017_06_07_10_19_47/model_252000.ckpt-252000\n",
      "average recall 0.471995477557\n",
      "average precision 0.632627025177\n",
      "hamming score 0.407507070691\n",
      "\n",
      "/checkpoints/gradient_001_124720_2017_06_07_10_19_49/model_162000.ckpt-162000\n",
      "average recall 0.494808333058\n",
      "average precision 0.646451136756\n",
      "hamming score 0.424510040521\n",
      "\n",
      "/checkpoints/gradient_001_112341_2017_05_15_22_53_23/model_80000.ckpt-80000\n",
      "average recall 0.498840689631\n",
      "average precision 0.6447054777\n",
      "hamming score 0.425482815963\n",
      "\n",
      "/checkpoints/gradient_001_112341_2017_05_15_22_53_23/model_56000.ckpt-56000\n",
      "average recall 0.491657750881\n",
      "average precision 0.637341647328\n",
      "hamming score 0.422113202254\n",
      "\n",
      "/checkpoints/gradient_001_112341_2017_05_29_17_55_48/model_42000.ckpt-42000\n",
      "average recall 0.500109116516\n",
      "average precision 0.631316360755\n",
      "hamming score 0.422887055574\n",
      "\n",
      "/attgrad_vgg_checkpoints/gradient_-05_144023_2017_07_27_03_55_12/model_244000.ckpt-244000\n",
      "average recall 0.478665626227\n",
      "average precision 0.61885935891\n",
      "hamming score 0.41045531007\n",
      "\n",
      "/attgrad_vgg_checkpoints/gradient_-05_144023_2017_07_27_03_55_03/model_244000.ckpt-244000\n",
      "average recall 0.486174096905\n",
      "average precision 0.629985326399\n",
      "hamming score 0.415480418202\n",
      "\n",
      "/attgrad_vgg_checkpoints/gradient_0001_144023_2017_07_27_03_55_01/model_240000.ckpt-240000\n",
      "average recall 0.466384530811\n",
      "average precision 0.617724201219\n",
      "hamming score 0.399192793518\n",
      "\n",
      "/attgrad_vgg_checkpoints/gradient_001_144023_2017_07_27_03_55_08/model_126000.ckpt-126000\n",
      "average recall 0.475612052198\n",
      "average precision 0.616802364261\n",
      "hamming score 0.408070788711\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import recall_score, precision_score\n",
    "model_results_db = pickledb.load('databases/linear_model_results.db', True)\n",
    "features_db = pickledb.load('databases/fc7_features_signature.db', True)\n",
    "\n",
    "L = !ls /media/data_cifs/danshiebler/data/generated_feature_vectors\n",
    "for l in sorted(L):\n",
    "    ID = l.split(\".\")[0]\n",
    "    features = features_db.get(ID)\n",
    "    model_results = model_results_db.get(ID)\n",
    "    preds = np.array(model_results['probs']) > 0.5\n",
    "    print features[\"saved_weights_path\"].split(\"/media/data_cifs/clicktionary/clickme_experiment\")[-1]\n",
    "    print \"average recall\", recall_score(np.array(model_results['y']), preds, average=\"samples\")\n",
    "    print \"average precision\", precision_score(np.array(model_results['y']), preds, average=\"samples\")\n",
    "    print \"hamming score\", hamming_score(model_results['y'], preds)\n",
    "    print "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.84      0.84      0.84     21634\n",
      "          1       0.42      0.24      0.31      1114\n",
      "          2       0.65      0.35      0.45      4180\n",
      "          3       0.88      0.44      0.59      1219\n",
      "          4       0.79      0.66      0.72       840\n",
      "          5       0.66      0.52      0.58      1350\n",
      "          6       0.84      0.65      0.74      1281\n",
      "          7       0.43      0.38      0.40      2056\n",
      "          8       0.58      0.51      0.54      1048\n",
      "          9       0.56      0.33      0.42      1437\n",
      "         10       0.41      0.39      0.40       592\n",
      "         11       0.73      0.25      0.37       589\n",
      "         12       0.65      0.14      0.23       261\n",
      "         13       0.56      0.17      0.27      1961\n",
      "         14       0.42      0.36      0.39      1121\n",
      "         15       0.72      0.67      0.70      1480\n",
      "         16       0.60      0.43      0.50      1521\n",
      "         17       0.62      0.42      0.50      1001\n",
      "         18       0.53      0.62      0.57       489\n",
      "         19       0.57      0.38      0.46       666\n",
      "         20       0.88      0.73      0.80       714\n",
      "         21       0.89      0.49      0.63       341\n",
      "         22       0.88      0.82      0.85       677\n",
      "         23       0.83      0.84      0.83       849\n",
      "         24       0.29      0.02      0.04      1832\n",
      "         25       0.57      0.34      0.43      1393\n",
      "         26       0.37      0.13      0.19      2272\n",
      "         27       0.62      0.34      0.44      1288\n",
      "         28       0.44      0.12      0.19       876\n",
      "         29       0.49      0.19      0.28       757\n",
      "         30       0.73      0.72      0.73       993\n",
      "         31       0.48      0.22      0.30       533\n",
      "         32       0.54      0.43      0.48      1445\n",
      "         33       0.51      0.66      0.58       727\n",
      "         34       0.63      0.46      0.53       799\n",
      "         35       0.69      0.58      0.63       845\n",
      "         36       0.62      0.33      0.43      1092\n",
      "         37       0.80      0.52      0.63      1292\n",
      "         38       0.90      0.61      0.72      1193\n",
      "         39       0.37      0.18      0.24      2912\n",
      "         40       0.28      0.39      0.33       872\n",
      "         41       0.50      0.33      0.40      3061\n",
      "         42       0.39      0.33      0.36      1173\n",
      "         43       0.37      0.29      0.32      1410\n",
      "         44       0.28      0.27      0.28      1189\n",
      "         45       0.45      0.39      0.42      2397\n",
      "         46       0.49      0.34      0.40       728\n",
      "         47       0.41      0.12      0.19       491\n",
      "         48       0.48      0.26      0.34       818\n",
      "         49       0.58      0.30      0.40       568\n",
      "         50       0.86      0.47      0.61       670\n",
      "         51       0.35      0.29      0.31       578\n",
      "         52       0.63      0.20      0.30       452\n",
      "         53       0.72      0.58      0.65      1117\n",
      "         54       0.62      0.09      0.16       523\n",
      "         55       0.53      0.23      0.32       969\n",
      "         56       0.55      0.28      0.37      4404\n",
      "         57       0.39      0.43      0.41      1448\n",
      "         58       0.47      0.10      0.16      1540\n",
      "         59       0.64      0.39      0.48      1292\n",
      "         60       0.59      0.53      0.56      3960\n",
      "         61       0.80      0.60      0.68      1185\n",
      "         62       0.64      0.30      0.40      1577\n",
      "         63       0.56      0.45      0.50      1232\n",
      "         64       0.62      0.46      0.52       674\n",
      "         65       0.41      0.16      0.23      1041\n",
      "         66       0.60      0.46      0.52       750\n",
      "         67       0.37      0.19      0.25      1695\n",
      "         68       0.50      0.12      0.19       512\n",
      "         69       0.51      0.47      0.49       989\n",
      "         70       0.10      0.04      0.06        74\n",
      "         71       0.77      0.12      0.21      1574\n",
      "         72       0.41      0.36      0.39       790\n",
      "         73       0.54      0.14      0.22      1828\n",
      "         74       0.78      0.41      0.54      1704\n",
      "         75       0.57      0.33      0.42      1200\n",
      "         76       0.22      0.10      0.14       302\n",
      "         77       0.56      0.41      0.47       724\n",
      "         78       0.00      0.00      0.00        70\n",
      "         79       0.28      0.15      0.19       341\n",
      "\n",
      "avg / total       0.61      0.45      0.50    116592\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print model_results[\"report\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
